{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "531ceab7",
   "metadata": {},
   "source": [
    "# Unzip the downloaded zip files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "05ecd50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import zipfile\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e6bc00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source folder: C:\\Users\\stgadmin\\Project\\Data downloads\\T100\n",
      "Destination folder: C:\\Users\\stgadmin\\Project\\Data downloads\\T100_Extractedcsvs\n",
      "Found 3 .wip file(s) to extract.\n",
      "\n",
      "Processing: T_T100_SEGMENT_ALL_CARRIER_20251022_145557_2025.zip\n",
      "‚úÖ 1 CSV file(s) moved to destination folder.\n",
      "\n",
      "Processing: T_T100_SEGMENT_ALL_CARRIER_20251022_145823_2024.zip\n",
      "‚úÖ 1 CSV file(s) moved to destination folder.\n",
      "\n",
      "Processing: T_T100_SEGMENT_ALL_CARRIER_20251022_150014_2023.zip\n",
      "‚úÖ 1 CSV file(s) moved to destination folder.\n",
      "\n",
      "All files processed.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 1. Define source and destination folders ---\n",
    "source_folder = r\"C:\\Users\\stgadmin\\Project\\Data downloads\\T100\"       # Folder containing your .wip files\n",
    "destination_folder = r\"C:\\Users\\stgadmin\\Project\\Data downloads\\T100_Extractedcsvs\"  # Folder where CSVs will go --- CORRECT\n",
    "\n",
    "print(f\"Source folder: {source_folder}\")\n",
    "print(f\"Destination folder: {destination_folder}\")\n",
    "\n",
    "\n",
    "# Ensure the destination folder exists\n",
    "os.makedirs(destination_folder, exist_ok=True)\n",
    "\n",
    "# --- 2. Find all .zip files in the source folder ---\n",
    "zip_files = glob.glob(os.path.join(source_folder, \"*.zip\"))\n",
    "\n",
    "if not zip_files:\n",
    "    print(\"No .wip files found in the source folder.\")\n",
    "else:\n",
    "    print(f\"Found {len(zip_files)} .wip file(s) to extract.\")\n",
    "\n",
    "    for zip_path in zip_files:\n",
    "        zip_name = os.path.basename(zip_path)\n",
    "        zip_base_name = os.path.splitext(zip_name)[0]\n",
    "        print(f\"\\nProcessing: {zip_name}\")\n",
    "\n",
    "        # Temporary folder to extract the .wip file\n",
    "        temp_extract_folder = os.path.join(source_folder, \"__temp_extract__\")\n",
    "        os.makedirs(temp_extract_folder, exist_ok=True)\n",
    "\n",
    "        try:\n",
    "            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "                zip_ref.extractall(temp_extract_folder)\n",
    "\n",
    "            # Move CSV files to destination folder\n",
    "            csv_files = glob.glob(os.path.join(temp_extract_folder, \"*.csv\"))\n",
    "            for csv_file in csv_files:\n",
    "                csv_ext = os.path.splitext(csv_file)[1]  # keep extension\n",
    "                new_csv_name = f\"{zip_base_name}{csv_ext}\"  # use only zip name\n",
    "                shutil.move(csv_file, os.path.join(destination_folder, new_csv_name))\n",
    "\n",
    "            print(f\"‚úÖ {len(csv_files)} CSV file(s) moved to destination folder.\")\n",
    "\n",
    "        except zipfile.BadZipFile:\n",
    "            print(f\"‚ùå Error: {zip_name} is not a valid zip file.\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Unexpected error while processing {zip_name}: {e}\")\n",
    "        finally:\n",
    "            # Clean up temporary folder\n",
    "            if os.path.exists(temp_extract_folder):\n",
    "                shutil.rmtree(temp_extract_folder)\n",
    "\n",
    "print(\"\\nAll files processed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316d4387",
   "metadata": {},
   "source": [
    "# Data Load to SQL Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90eee9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import urllib\n",
    "from sqlalchemy import create_engine, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313c6d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 CSV file(s) in the folder.\n",
      "\n",
      "üìÑ Loading new file: T_T100_SEGMENT_ALL_CARRIER_2023.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stgadmin\\AppData\\Local\\Temp\\ipykernel_19760\\1695617586.py:50: DtypeWarning: Columns (13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(csv_file, chunksize=chunksize):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ File successfully inserted and logged: T_T100_SEGMENT_ALL_CARRIER_2023.csv\n",
      "\n",
      "üìÑ Loading new file: T_T100_SEGMENT_ALL_CARRIER_2024.csv\n",
      "‚úÖ File successfully inserted and logged: T_T100_SEGMENT_ALL_CARRIER_2024.csv\n",
      "\n",
      "üìÑ Loading new file: T_T100_SEGMENT_ALL_CARRIER_2025.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stgadmin\\AppData\\Local\\Temp\\ipykernel_19760\\1695617586.py:50: DtypeWarning: Columns (13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(csv_file, chunksize=chunksize):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ File successfully inserted and logged: T_T100_SEGMENT_ALL_CARRIER_2025.csv\n",
      "\n",
      " All new CSV files have been processed and uploaded to SQL Server.\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Define folder containing CSVs ---\n",
    "folder_path = r\"C:\\Users\\stgadmin\\Project\\air-travel-dwh\\Data\\T100\"\n",
    "csv_files = glob.glob(os.path.join(folder_path, \"*.csv\"))\n",
    "\n",
    "if not csv_files:\n",
    "    print(\"No CSV files found.\")\n",
    "else:\n",
    "    print(f\"Found {len(csv_files)} CSV file(s) in the folder.\")\n",
    "\n",
    "# --- 2. Database connection parameters ---\n",
    "params = urllib.parse.quote_plus(\n",
    "    \"DRIVER={ODBC Driver 17 for SQL Server};\"\n",
    "    \"SERVER=ICT-210-11;\"\n",
    "    \"DATABASE=AirtravelBronze;\"\n",
    "    \"Trusted_Connection=yes;\"\n",
    "    \"Encrypt=yes;\"\n",
    "    \"TrustServerCertificate=yes;\"\n",
    ")\n",
    "connection_url = f\"mssql+pyodbc:///?odbc_connect={params}\"\n",
    "engine_destination = create_engine(connection_url, fast_executemany=True)\n",
    "\n",
    "# --- 3. Ensure log table exists ---\n",
    "with engine_destination.begin() as conn:\n",
    "    conn.execute(text(\"\"\"\n",
    "        IF NOT EXISTS (SELECT * FROM sys.tables WHERE name = 'T100_Load_Log')\n",
    "        BEGIN\n",
    "            CREATE TABLE dbo.T100_Load_Log (\n",
    "                FileName NVARCHAR(255) PRIMARY KEY,\n",
    "                LoadDate DATETIME DEFAULT GETDATE()\n",
    "            );\n",
    "        END\n",
    "    \"\"\"))\n",
    "\n",
    "# --- 4. Function to load a single CSV if not already loaded ---\n",
    "def load_csv_to_sql(csv_file, table_name='T_100_MonthlyData', chunksize=50000):\n",
    "    csv_name = os.path.basename(csv_file)\n",
    "    with engine_destination.begin() as conn:\n",
    "        # Check if file has been loaded\n",
    "        result = conn.execute(\n",
    "            text(\"SELECT COUNT(*) FROM dbo.T100_Load_Log WHERE FileName = :fname\"),\n",
    "            {\"fname\": csv_name}\n",
    "        ).scalar()\n",
    "\n",
    "        if result > 0:\n",
    "            print(f\"‚ö†Ô∏è File already loaded, skipping: {csv_name}\")\n",
    "            return\n",
    "\n",
    "        # Load CSV in chunks\n",
    "        print(f\"\\nüìÑ Loading new file: {csv_name}\")\n",
    "        for chunk in pd.read_csv(csv_file, chunksize=chunksize):\n",
    "            chunk.to_sql(\n",
    "                name=table_name,\n",
    "                con=conn,\n",
    "                if_exists='append',\n",
    "                index=False\n",
    "            )\n",
    "\n",
    "        # Log the loaded file\n",
    "        conn.execute(\n",
    "            text(\"INSERT INTO dbo.T100_Load_Log (FileName) VALUES (:fname)\"),\n",
    "            {\"fname\": csv_name}\n",
    "        )\n",
    "        print(f\"‚úÖ File successfully inserted and logged: {csv_name}\")\n",
    "\n",
    "# --- 5. Process all CSVs ---\n",
    "for csv_file in csv_files:\n",
    "    load_csv_to_sql(csv_file)\n",
    "\n",
    "print(\"\\n All new CSV files have been processed and uploaded to SQL Server.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Data_Engineering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
